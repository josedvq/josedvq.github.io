<!doctype html>
<html lang="en" dir="ltr" class="mdx-wrapper mdx-page plugin-pages plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.1.0">
<title data-rh="true">Projects | Jose Vargas-Quiros</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://josedvq.github.io/projects"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Projects | Jose Vargas-Quiros"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://josedvq.github.io/projects"><link data-rh="true" rel="alternate" href="https://josedvq.github.io/projects" hreflang="en"><link data-rh="true" rel="alternate" href="https://josedvq.github.io/projects" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Jose Vargas-Quiros RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Jose Vargas-Quiros Atom Feed">



<link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.2.0/css/fontawesome.css" media="print" onload="this.media=&quot;all&quot;">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.2.0/css/all.css" media="print" onload="this.media=&quot;all&quot;">
<script src="https://cdn.lightwidget.com/widgets/lightwidget.js"></script><link rel="stylesheet" href="/assets/css/styles.b2da171a.css">
<link rel="preload" href="/assets/js/runtime~main.ffa9b863.js" as="script">
<link rel="preload" href="/assets/js/main.a6941c66.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="theme.common.skipToMainContent"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/me.jpg" alt="Jose David Vargas" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/me.jpg" alt="Jose David Vargas" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Jose Vargas</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" docid="projects" href="/projects">Projects</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" docid="cv" href="/cv">CV</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/josedvq" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper mainWrapper_z2l0"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_j9I6"><div class="col col--8"><article><section class="container_c17D"><div class="imgContainer_arMn"><img src="/img/conflab.jpg" class="img_K7ni" alt="dummy img"></div><div class="content_A65C"><div class="titleContainer_h5La"><span class="title_EaXC">The ConfLab Social Interaction Dataset</span></div><span class="description_qJJf"><p>ConfLab is a privacy-sensitive data collection concept and dataset for in-the-wild social interactions. We recorded free interaction of 48 subjects during a networking event at ACM Multimedia 2019. Our capture setup improves upon the data fidelity of prior in-the-wild datasets while retaining privacy sensitivity. We recorded videos from a non-invasive overhead view. Via chest-worn wearable sensors, we recorded body motion (9-axis IMU) low-frequency audio (1250 Hz), and Bluetooth-based proximity.</p><p style="text-align:center"><a href="https://josedvq.github.io/conflab-web" target="_blank" rel="noopener noreferrer" class="button button--primary">To ConfLab dataset</a></p><p>Our benchmarks tasks showcase some of the open research tasks related to in-the-wild privacy-preserving social data analysis: keypoints detection from overhead camera views (via fine-tuned Mask-RCNN), skeleton-based no-audio speaker detection (vi), and F-formation detection. The ConfLab project resulted in the following publications.</p></span><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><a class="url_uCqg" href="https://doi.org/10.48550/arXiv.2205.05177" target="_blank"><i class="fa-solid fa-link"></i></a><a class="title_UFdU" href="https://doi.org/10.48550/arXiv.2205.05177" target="_blank">ConfLab: A Data Collection Concept, Dataset, and Benchmark for Machine Analysis of Free-Standing Social Interactions in the Wild</a><span class="date_O8gS">2022</span><span class="authors__36R">Chirag Raman*, Jose Vargas-Quiros*, Stephanie Tan*, Ekin Gedik, Ashraful Islam, and Hayley Hung</span><span class="publication_rgyV">NeurIPS 2022 Datasets and Benchmarks Track</span></div></div><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><a class="url_uCqg" href="https://dl-acm-org.tudelft.idm.oclc.org/doi/10.1145/3343031.3351320" target="_blank"><i class="fa-solid fa-link"></i></a><a class="title_UFdU" href="https://dl-acm-org.tudelft.idm.oclc.org/doi/10.1145/3343031.3351320" target="_blank">Multimodal Data Collection for Social Interaction Analysis In-the-Wild</a><span class="date_O8gS">2019</span><span class="authors__36R">Hayley Hung, Chirag Raman, Ekin Gedik, Stephanie Tan and Jose Vargas</span><span class="publication_rgyV">MM &#x27;19 - Proceedings of the 27th ACM International Conference on Multimedia</span></div></div><div class="linkContainer_NQ6x"><a style="margin-right:.7rem;margin-left:-.7rem;padding:.5rem .7rem" href="https://github.com/TUDelft-SPC-Lab/conflab" target="_blank">Code</a><a style="margin-right:.7rem;margin-left:-.7rem;padding:.5rem .7rem" href="https://josedvq.github.io/conflab-web" target="_blank">Live</a></div></div></section><section class="container_c17D"><div class="content_A65C"><div class="titleContainer_h5La"><span class="title_EaXC">Differences in the annotation of laughter across modalities </span></div><span class="description_qJJf">Although laughter is well-recognized as a multimodal phenomenon, it is unclear how annotation of laughter differs when done from modalities like video, without access to audio. In this work we take a first step in this direction by asking if and how well laughter can be annotated when only audio, only video (containing full body movement information) or audiovisual modalities are available to annotators. We ask whether annotations of laughter are congruent across modalities, and compare the effect that labeling modality has on machine learning model performance. We compare annotations and models for laughter detection, intensity estimation, and segmentation, three tasks common in previous studies of laughter. Our analysis makes use of more than 4000 annotations acquired from 48 annotators, making use of the Covfee annotation framework.</span><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><span class="title_UFdU">Differences in annotation of laughter across modalities</span><span class="date_O8gS">2022</span><span class="authors__36R">Jose Vargas-Quiros, Laura Cabrera-Quiros, Catharine Oertel, and Hayley Hung</span><span class="publication_rgyV">Under review</span></div></div><div class="linkContainer_NQ6x"><a style="margin-right:.7rem;margin-left:-.7rem;padding:.5rem .7rem" href="https://github.com/josedvq/lared-laughter" target="_blank">Code</a></div></div></section><section class="container_c17D"><div class="imgContainer_arMn"><img src="/img/covfee.jpg" class="img_K7ni" alt="dummy img"></div><div class="content_A65C"><div class="titleContainer_h5La"><span class="title_EaXC">Covfee: an extensible web framework for continuous-time annotation of human behavior</span></div><span class="description_qJJf">During my PhD, I developed a web-based, extensible framework for continuous annotation aimed at crowd-sourcing. Our experiments showed that continuous techniques (implemented in Covfee) can save human annotation time with no loss in annotation agreement.<p style="text-align:center"><a href="https://github.com/josedvq/covfee" target="_blank" rel="noopener noreferrer" class="button button--primary">To Covfee docs</a></p></span><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><a class="url_uCqg" href="https://proceedings.mlr.press/v173/vargas-quiros22a.html" target="_blank"><i class="fa-solid fa-link"></i></a><a class="title_UFdU" href="https://proceedings.mlr.press/v173/vargas-quiros22a.html" target="_blank">Covfee: an extensible web framework for continuous-time annotation of human behavior</a><span class="date_O8gS">2022</span><span class="authors__36R">Jose Vargas-Quiros, Stephanie Tan, Chirag Raman, Laura Cabrera-Quiros, and Hayley Hung</span><span class="publication_rgyV">Understanding Social Behavior in Dyadic and Small Group Interactions, Proceedings of Machine Learning Research</span></div></div><div class="linkContainer_NQ6x"><a style="margin-right:.7rem;margin-left:-.7rem;padding:.5rem .7rem" href="https://github.com/josedvq/covfee" target="_blank">Code</a><a style="margin-right:.7rem;margin-left:-.7rem;padding:.5rem .7rem" href="https://josedvq.github.io/covfee/" target="_blank">Live</a></div></div></section><section class="container_c17D"><div class="content_A65C"><div class="titleContainer_h5La"><span class="title_EaXC">Attraction and body movement in speed dates</span></div><span class="description_qJJf">We present a study of attraction in the dyadic speed date setting. The study made use of accelerometer information (from chest-worn accelerometers) from 398 dyadic speed dates to analyze the relationship between body movement and self-reported affiliative goals related to attraction. Through machine learning experiments designed to capture individual and pairwise body movement information, we investigated the predictive power of body movement information towards attraction estimation. In particular, the pairwise features used in our study were designed to capture synchrony, mimicry and convergence information.</span><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><a class="url_uCqg" href="https://ieeexplore.ieee.org/document/9662986" target="_blank"><i class="fa-solid fa-link"></i></a><a class="title_UFdU" href="https://ieeexplore.ieee.org/document/9662986" target="_blank">Individual and joint body movement assessed by wearable sensing as a predictor of attraction in speed dates</a><span class="date_O8gS">2021</span><span class="authors__36R">Jose Vargas-Quiros, Oyku Kacap, Hayley Hung, Laura Cabrera-Quiros</span><span class="publication_rgyV">IEEE Transactions on Affective Computing</span></div></div><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><a class="url_uCqg" href="https://ieeexplore.ieee.org/document/8925137" target="_blank"><i class="fa-solid fa-link"></i></a><a class="title_UFdU" href="https://ieeexplore.ieee.org/document/8925137" target="_blank">Estimating Romantic, Social, and Sexual Attraction by Quantifying Bodily Coordination using Wearable Sensors</a><span class="date_O8gS">2019</span><span class="authors__36R">Oyku Kacap, Jose Vargas-Quiros, Hayley Hung</span><span class="publication_rgyV">International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</span></div></div><div class="linkContainer_NQ6x"></div></div></section><section class="container_c17D"><div class="content_A65C"><div class="titleContainer_h5La"><span class="title_EaXC">No-audio speaking status detection in crowded settings via visual pose-based filtering and wearable acceleration</span></div><span class="description_qJJf">Recognizing who is speaking in a crowded scene is a key challenge towards the understanding of the social interactions going on within it. Unfortunately individual audio recordings are not available in many social interaction datasets due to subject privacy and logistic challenges. However, video and wearable sensors make it possible to recognize speaking in an unobtrusive, privacy-preserving way through body movement information. When considering the video modality, a bounding box is traditionally used in action recognition problems to localize the target subject whose action is to be assessed. However, cross-contamination, occlusion, and the articulated nature of the human body, make bounding boxes unsuitable in crowded scenes. We address this problem via a method making use of articulated body poses for subject localization and in the subsequent speech detection stage. We show that the selection of local features around pose keypoints has a positive effect on generalization performance while also significantly reducing the number of local features considered, making for a more efficient method. Using two in-the-wild datasets with different viewpoints of subjects, we investigate the role of cross-contamination in this effect. We additionally make use of acceleration measured through wearable sensors for the same task, and present a multimodal approach combining both methods.</span><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><a class="url_uCqg" href="https://arxiv.org/abs/2211.00549" target="_blank"><i class="fa-solid fa-link"></i></a><a class="title_UFdU" href="https://arxiv.org/abs/2211.00549" target="_blank">No-audio speaking status detection in crowded settings via visual pose-based filtering and wearable acceleration</a><span class="date_O8gS">2019</span><span class="authors__36R">Jose Vargas-Quiros, Laura Cabrera-Quiros, Hayley Hung</span></div></div><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><a class="url_uCqg" href="https://dl-acm-org.tudelft.idm.oclc.org/doi/10.1145/3343031.3351320" target="_blank"><i class="fa-solid fa-link"></i></a><a class="title_UFdU" href="https://dl-acm-org.tudelft.idm.oclc.org/doi/10.1145/3343031.3351320" target="_blank">Multimodal Data Collection for Social Interaction Analysis In-the-Wild</a><span class="date_O8gS">2019</span><span class="authors__36R">Hayley Hung, Chirag Raman, Ekin Gedik, Stephanie Tan and Jose Vargas</span><span class="publication_rgyV">MM &#x27;19 - Proceedings of the 27th ACM International Conference on Multimedia</span></div></div><div class="linkContainer_NQ6x"></div></div></section><section class="container_c17D"><div class="content_A65C"><div class="titleContainer_h5La"><span class="title_EaXC">LaRed dataset: a mingling dataset with high-quality individual audio</span></div><span class="description_qJJf">Recognizing voice activity from human speakers in a multimodal recording is a central task towards the understanding of the social interactions occurring within it. LaRed is an in-the-wild dataset for the study of voice activity from body movement. For ground truth, our dataset contains high-fidelity audio recordings from individual Lavalier microphones worn by subjects in the scene. The dataset also contains automatically-extracted pose tracks and chest-worn accelerometer readings, which provide an indication of overall body movement. We present three baselines for no-audio voice activity detection: a) voice activity detection from video, b) voice activity detection from body acceleration (chest-worn accelerometer), c) voice activity detection from our noisy pose tracks. The LaRed dataset provides the signals and ground truth necessary to evaluate a wide range of methods for voice activity detection from body movements.</span><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><span class="title_UFdU">LaRed Dataset: no-audio speaking status detection in a crowded-setting</span><span class="date_O8gS">2022</span><span class="authors__36R">Jose Vargas-Quiros, Hayley Hung, and Laura Cabrera-Quiros</span><span class="publication_rgyV">Work in progress</span></div></div><div class="linkContainer_NQ6x"></div></div></section><section class="container_c17D"><div class="content_A65C"><div class="titleContainer_h5La"><span class="title_EaXC">Information-theoretic anomaly detection and authorship attribution in literature (MSc thesis)</span></div><span class="description_qJJf">KRIMP is an algorithm based on information theory capable of capturing arbitrary length co-occurrence relations between itemsets in a database. KRIMP code tables have shown to be useful models of databases for multiple machine learning tasks including classification and clustering. Cross-compression sizes obtained from KRIMP code tables are a generalization of cross-entropy capable of taking into account such co-occurrence relations. This work focuses on the application of KRIMP cross-compression in two different but overlapping areas. First, an unsupervised anomalous database detection algorithm is presented, capable of taking into account database structure. The algorithm is tested on itemset databases with a significant amount of structure to characterize its behavior and an experiment is performed on text data. Second, the application of KRIMP to natural language is investigated further, in the context of authorship attribution via classification. The KRIMP classifier is a generalization of the Naive Bayes classifier capable of combining frequently co-occurring words or items into itemsets, delivering an appealing comparison between the two algorithms. Different ways of transforming text into itemset form were explored, as well as two different ways of applying Laplace smoothing. Experiments indicated that co-occurrence relations are important for attribution, with KRIMP being more robust and in most cases more accurate on prose text when the complete alphabet is considered and itemsets are created per sentence. The behavior suggests that there is relevant syntactic structure at the sentence level involving punctuation and function words that is captured in KRIMP code tables, and is characteristic of individual authors.</span><div style="margin:1em auto"><style data-emotion="css p9qzma">.css-p9qzma{color:rgba(0, 0, 0, 0.6);}</style><style data-emotion="css 8musp7">.css-8musp7{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border-radius:4px;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:rgba(0, 0, 0, 0.6);}</style><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation2 paperContainer_owZE css-8musp7"><a class="url_uCqg" href="https://josedvq.github.io/msc_thesis.pdf" target="_blank"><i class="fa-solid fa-link"></i></a><a class="title_UFdU" href="https://josedvq.github.io/msc_thesis.pdf" target="_blank">Information-theoretic anomaly detection and authorship attribution in literature</a><span class="date_O8gS">2017</span><span class="authors__36R">Jose Vargas-Quiros</span><span class="publication_rgyV">Universiteit Utrecht</span></div></div><div class="linkContainer_NQ6x"></div></div></section></article></div></div></main></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Social Media</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://twitter.com/josedvq" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/josedvq" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://tungsten-hedge-174.notion.site/Recipes-32ae4a9baac6463992487190d5baa0ff" target="_blank" rel="noopener noreferrer" class="footer__link-item">My recipes blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 Jose Vargas. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.ffa9b863.js"></script>
<script src="/assets/js/main.a6941c66.js"></script>
</body>
</html>